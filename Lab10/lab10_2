1. What does AdaGrad do to boost performance?

    Adagrad is one of the optimizers that modifies the learning rate adaptively for each coefficient in a model, monotonically 
    lowering the effective learning rate. It adapts the learning rate to the parameters, performing smaller updates for paremeters
    associated with frequently occurring features, and larger updates for parameters associated with infrequent features which
    makes it suitable for dealing with sparse data. It is useful for convex problems Neural Net training.
    
    
2. Tasks 1â€“4: Report your best hyperparameter settings and their resulting performance.


    <Task 1>
    def normalize_linear_scale(examples_dataframe):
      """Returns a version of the input `DataFrame` that has all its features normalized linearly."""
      processed_features = pd.DataFrame()
      processed_features["latitude"] = linear_scale(examples_dataframe["latitude"])
      processed_features["longitude"] = linear_scale(examples_dataframe["longitude"])
      processed_features["housing_median_age"] = linear_scale(examples_dataframe["housing_median_age"])
      processed_features["total_rooms"] = linear_scale(examples_dataframe["total_rooms"])
      processed_features["total_bedrooms"] = linear_scale(examples_dataframe["total_bedrooms"])
      processed_features["population"] = linear_scale(examples_dataframe["population"])
      processed_features["households"] = linear_scale(examples_dataframe["households"])
      processed_features["median_income"] = linear_scale(examples_dataframe["median_income"])
      processed_features["rooms_per_person"] = linear_scale(examples_dataframe["rooms_per_person"])
      return processed_features

    normalized_dataframe = normalize_linear_scale(preprocess_features(california_housing_dataframe))
    normalized_training_examples = normalized_dataframe.head(12000)
    normalized_validation_examples = normalized_dataframe.tail(5000)

    _ = train_nn_regression_model(
        my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.007),
        steps=5000,
        batch_size=70,
        hidden_units=[10, 10],
        training_examples=normalized_training_examples,
        training_targets=training_targets,
        validation_examples=normalized_validation_examples,
        validation_targets=validation_targets)
        
       Final RMSE (on training data):   66.68
       Final RMSE (on validation data): 66.79
       
    <Task 2>
    
    #Adagrad
    _, adagrad_training_losses, adagrad_validation_losses = train_nn_regression_model(
        my_optimizer=tf.train.AdagradOptimizer(learning_rate=0.7),
        steps=700,
        batch_size=150,
        hidden_units=[10, 10],
        training_examples=normalized_training_examples,
        training_targets=training_targets,
        validation_examples=normalized_validation_examples,
        validation_targets=validation_targets)

    #Adam
    _, adam_training_losses, adam_validation_losses = train_nn_regression_model(
        my_optimizer=tf.train.AdamOptimizer(learning_rate=0.007),
        steps=700,
        batch_size=150,
        hidden_units=[10, 10],
        training_examples=normalized_training_examples,
        training_targets=training_targets,
        validation_examples=normalized_validation_examples,
        validation_targets=validation_targets)

    #Print out
    plt.ylabel("RMSE")
    plt.xlabel("Periods")
    plt.title("Root Mean Squared Error vs. Periods")
    plt.plot(adagrad_training_losses, label='Adagrad training')
    plt.plot(adagrad_validation_losses, label='Adagrad validation')
    plt.plot(adam_training_losses, label='Adam training')
    plt.plot(adam_validation_losses, label='Adam validation')
    _ = plt.legend()

    Adagrad
    Final RMSE (on training data):   68.29
    Final RMSE (on validation data): 68.59
    
    Adam
    Final RMSE (on training data):   69.20
    Final RMSE (on validation data): 69.52
    
    <Task 3>
    def normalize(examples_dataframe):
      """Returns a version of the input `DataFrame` that has all its features normalized."""
      processed_features = pd.DataFrame()

      processed_features["households"] = log_normalize(examples_dataframe["households"])
      processed_features["median_income"] = log_normalize(examples_dataframe["median_income"])
      processed_features["total_bedrooms"] = log_normalize(examples_dataframe["total_bedrooms"])

      processed_features["latitude"] = linear_scale(examples_dataframe["latitude"])
      processed_features["longitude"] = linear_scale(examples_dataframe["longitude"])
      processed_features["housing_median_age"] = linear_scale(examples_dataframe["housing_median_age"])

      processed_features["population"] = linear_scale(clip(examples_dataframe["population"], 0, 5000))
      processed_features["rooms_per_person"] = linear_scale(clip(examples_dataframe["rooms_per_person"], 0, 5))
      processed_features["total_rooms"] = linear_scale(clip(examples_dataframe["total_rooms"], 0, 10000))

      return processed_features

    normalized_dataframe = normalize(preprocess_features(california_housing_dataframe))
    normalized_training_examples = normalized_dataframe.head(12000)
    normalized_validation_examples = normalized_dataframe.tail(5000)

    _ = train_nn_regression_model(
        my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.17),
        steps=1000,
        batch_size=70,
        hidden_units=[10, 10],
        training_examples=normalized_training_examples,
        training_targets=training_targets,
        validation_examples=normalized_validation_examples,
        validation_targets=validation_targets)
        
    Final RMSE (on training data):   102.25
    Final RMSE (on validation data): 101.58
