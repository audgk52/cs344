Questions:

Submit solutions to tasks 1â€“5.
  
  Task #1
  
  Max and min value (15000 and 500000) from median_house_value seem like it is fixed artificially. Also, rooms per person value
  showing 55.2 (training dataset) and 18.3 (validation dataset) seems odd (altohought there might be a house with a lot of rooms
  where only one person lived). 
  
  Task #2
  
  The distribution of training and validation data are quite off (which they should be similar). As the biggest problem in the plot
  created with the validation data, the latitude and longitude summary stat is very different than the ones of training data which 
  shows quite accurate plot compared to a real map of California. 
    
  Task #3
  
  california_housing_dataframe = california_housing_dataframe.reindex(
    np.random.permutation(california_housing_dataframe.index))
   
  Uncommenting randomization code would fix the bug in this case since it will randomize the data before creating training and 
  validation splits which can prevent getting sorted data in order. 
  
  Task #4
  
  def train_model(
    learning_rate,
    steps,
    batch_size,
    training_examples,
    training_targets,
    validation_examples,
    validation_targets):
  """Trains a linear regression model of multiple features.
  
  In addition to training, this function also prints training progress information,
  as well as a plot of the training and validation loss over time.
  
  Args:
    learning_rate: A `float`, the learning rate.
    steps: A non-zero `int`, the total number of training steps. A training step
      consists of a forward and backward pass using a single batch.
    batch_size: A non-zero `int`, the batch size.
    training_examples: A `DataFrame` containing one or more columns from
      `california_housing_dataframe` to use as input features for training.
    training_targets: A `DataFrame` containing exactly one column from
      `california_housing_dataframe` to use as target for training.
    validation_examples: A `DataFrame` containing one or more columns from
      `california_housing_dataframe` to use as input features for validation.
    validation_targets: A `DataFrame` containing exactly one column from
      `california_housing_dataframe` to use as target for validation.
      
  Returns:
    A `LinearRegressor` object trained on the training data.
  """

  periods = 10
  steps_per_period = steps / periods
  
  # Create a linear regressor object.
  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
  linear_regressor = tf.estimator.LinearRegressor(
      feature_columns=construct_feature_columns(training_examples),
      optimizer=my_optimizer
  )
  
  # 1. Create input functions.
  training_input_fn = lambda: my_input_fn(
      training_examples, 
      training_targets["median_house_value"], 
      batch_size=batch_size)
  predict_training_input_fn = lambda: my_input_fn(
      training_examples, 
      training_targets["median_house_value"], 
      num_epochs=1, 
      shuffle=False)
  predict_validation_input_fn = lambda: my_input_fn(
      validation_examples, validation_targets["median_house_value"], 
      num_epochs=1, 
      shuffle=False)
  
  # Train the model, but do so inside a loop so that we can periodically assess
  # loss metrics.
  print("Training model...")
  print("RMSE (on training data):")
  training_rmse = []
  validation_rmse = []
  for period in range (0, periods):
    # Train the model, starting from the prior state.
    linear_regressor.train(
        input_fn=training_input_fn,
        steps=steps_per_period,
    )
    # 2. Take a break and compute predictions.
    training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)
    training_predictions = np.array([item['predictions'][0] for item in training_predictions])
    
    validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)
    validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])
    
    # Compute training and validation loss.
    training_root_mean_squared_error = math.sqrt(
        metrics.mean_squared_error(training_predictions, training_targets))
    validation_root_mean_squared_error = math.sqrt(
        metrics.mean_squared_error(validation_predictions, validation_targets))
    # Occasionally print the current loss.
    print("  period %02d : %0.2f" % (period, training_root_mean_squared_error))
    # Add the loss metrics from this period to our list.
    training_rmse.append(training_root_mean_squared_error)
    validation_rmse.append(validation_root_mean_squared_error)
  print("Model training finished.")

  # Output a graph of loss metrics over periods.
  plt.ylabel("RMSE")
  plt.xlabel("Periods")
  plt.title("Root Mean Squared Error vs. Periods")
  plt.tight_layout()
  plt.plot(training_rmse, label="training")
  plt.plot(validation_rmse, label="validation")
  plt.legend()

  return linear_regressor
  
  linear_regressor = train_model(
    # TWEAK THESE VALUES TO SEE HOW MUCH YOU CAN IMPROVE THE RMSE
    learning_rate=0.00002,
    steps=1000,
    batch_size=10,
    training_examples=training_examples,
    training_targets=training_targets,
    validation_examples=validation_examples,
    validation_targets=validation_targets)
    
  RMSE (on training data):
      period 00 : 201.66
      period 01 : 181.50
      period 02 : 169.82
      period 03 : 163.68
      period 04 : 161.40
      period 05 : 160.87
      period 06 : 161.16
      period 07 : 161.80
      period 08 : 162.54
      period 09 : 162.71
      
  RMSE on training dataset shows better results than RMSE on validation dataset.
  
  Task #5
  
  california_housing_test_data = pd.read_csv("https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv", sep=",")

  test_examples = preprocess_features(california_housing_test_data)
  test_targets = preprocess_targets(california_housing_test_data)

  predict_test_input_fn = lambda: my_input_fn(
        test_examples, 
        test_targets["median_house_value"], 
        num_epochs=1, 
        shuffle=False)

  test_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)
  test_predictions = np.array([item['predictions'][0] for item in test_predictions])

  root_mean_squared_error = math.sqrt(
      metrics.mean_squared_error(test_predictions, test_targets))

  print("Final RMSE (on test data): %0.2f" % root_mean_squared_error)

  Final RMSE (on test data): 160.75
  
  
  
  
  
Give a one-paragraph summary of what you learned about using training, validation and testing datasets.

  Using training, valudation adn testing datasets improves the accuracy of data represetation. First, choosing larger randomized
  (so that we don't get sorted data which can create incorrect data representation)portion of data from a dataset to create the 
  training set and choosing smaller randomized portion of data to create validation dataset can let us compare those two sets of 
  data which should be roughly equal. After comparing training and validation dataset with many iteration, we compare it with 
  test dataset to see if there is an overfit. 
  
